{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [A2] Data Preparation and Regression\n",
    "In this notebook the learned foundations of the **Pandas** library are applied to a **practical example**. \n",
    "\n",
    "The aim of this assignment is to first **examine** a dataset using an **Exploratory Data Analysis**. We want to get to know the data and analyze it using an exploratory approach. As part of this process, several methods for **dataset preparation** are introduced.\n",
    "\n",
    "Second, the preprocessed dataset will be used to train **regression models** that **predict the monthly cold rent** based on various features such as size and condition of the apartment.\n",
    "\n",
    "## 1: About the Dataset\n",
    "A user of the data science community Kaggle developed and ran a web-scraper for the biggest **real estate platform** in Germany, **ImmoScout24**.\n",
    "\n",
    "At three points in time (22th Sept 2018, 10th Mai 2019 and 8th Oct. 2019), all available offers at that time were retrieved from the website and stored. \n",
    "In total, data samples of over 200.000 rental properties have been collected.\n",
    "\n",
    "The data samples contain the most important characteristics of a rental property, such as the size of the living space, the rent, both cold rent and total rent (if applicable), the location (street and house number, if available, zip code and state), energy type, etc. \n",
    "There are also two variables that contain longer free text descriptions: Description with text describing what is offered, and Amenities with a description of all available amenities, recent renovation, etc. The date column was added to indicate the time of scraping.\n",
    "\n",
    "The dataset _immo_data.csv_ can be imported from the 'data' directory or downloaded [from kaggle](https://www.kaggle.com/corrieaar/apartment-rental-offers-in-germany) (account required). An additional file _immo_data_column_description.csv_ contains explanation for each column name."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from sklearn import linear_model\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split\n",
    "\n",
    "pd.options.display.max_columns = 50\n",
    "\n",
    "immo = pd.read_csv(\"data/immo_data.csv.zip\")\n",
    "column_descriptions = pd.read_csv(\"data/immo_data_column_description.csv\")\n",
    "\n",
    "immo.head(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As described in the first notebook, some info about the DataFrame and statistical properties of its columns can be automatically generated:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "display(immo.info(), immo.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: First Impressions\n",
    "At first glance, just by looking at the list of columns, some columns seem to be duplicate.\n",
    "\n",
    "Besides, we can already identify some columns that are expected to not be helpful for later classification problems. In practice however, you would need to verify this first!\n",
    "\n",
    "The following code removes these columns. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "immo_2 = immo.drop([\"scoutId\", \"houseNumber\", \"geo_bln\", \"geo_krs\", \"geo_plz\", \"date\"], axis=1)\n",
    "immo_2.head(3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1: ✏️ Characteristics of the Dataset\n",
    "1.  How large is the (column-reduced) dataset? How many features (columns) or objects (data rows) are available?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "print(f'num_cols {len(immo_2.columns)}')\n",
    "print(f'num_rows {len(immo_2.index)}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Output the first ten objects. Ask yourself: How do they look like? What do you notice? Do you understand the meaning of the features?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "immo_2.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. What are the data types of each feature?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "print(immo_2.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. What are the respective maximum, average and minimum values of the following features?\n",
    "- telekomUploadSpeed\n",
    "- livingSpace\n",
    "- heatingCosts"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "print(f'telekomUploadSpeed ∈ [{immo_2['telekomUploadSpeed'].min()}, {immo_2['telekomUploadSpeed'].max()}], x̄ = {immo_2[\"telekomUploadSpeed\"].mean()}')\n",
    "print(f'livingSpace ∈ [{immo_2['livingSpace'].min()}, {immo_2['livingSpace'].max()}], x̄ = {immo_2[\"livingSpace\"].mean()}')\n",
    "print(f'heatingCosts ∈ [{immo_2['heatingCosts'].min()}, {immo_2['heatingCosts'].max()}], x̄ = {immo_2[\"heatingCosts\"].mean()}')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the analyzing process into two parts, each regarding either columns of numeric or non-numeric data type. \n",
    "\n",
    "## 3: Analyzing Categorical Data\n",
    "We start with the non-numeric data columns by taking a look at their statistical characteristics: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "immo_2.describe(exclude=np.number).T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _count_ column shows the number of values in our dataset. If it is lower than our number of data samples, _NaN_ (null) values occur.\n",
    "\n",
    "The _unique_ columns counts the number of distinct values. \n",
    "\n",
    "Those values that are represented the most for each column are mentioned as _top_ in the description table. The _freq_ is this most common value’s frequency.\n",
    "\n",
    "### 3.1: One Hot Encoding\n",
    "\n",
    "When categorical features are transformed using One Hot Encoding, a new column is created for each distinct value, as shown in the following example.\n",
    "\n",
    "This is required for using categorical features in a numeric regression model.\n",
    "\n",
    "#### 3.1.2: Exemplary One Hot Encoding\n",
    "\n",
    "In the following cell the column _C2_ of the DataFrame is one-hot encoded, generating new columns that replace the original column _C2_."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "example_df = pd.DataFrame({\"C1\": [\"a\", \"b\", \"a\", \"e\"], \"C2\": [\"b\", \"a\", \"c\", \"a\"]})\n",
    "example_df_oh = pd.get_dummies(example_df, columns=[\"C2\"])\n",
    "display(example_df, example_df_oh)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2: ✏️ Removing Inappropriate Columns for One Hot Encoding\n",
    "If we would want to one-hot encode the free text feature _description_, we would get a DataFrame with over 200,000 columns - so we exclude attributes with too many values for the time being.\n",
    "\n",
    "1. In the ``immo_2`` DataFrame, identify the features that could cause too many columns when one-hot encoded and create a DataFrame ``immo_3`` that does not contain these columns"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "immo_3 = immo_2.drop(columns=['street','streetPlain','regio2','regio3','description','facilities'])\n",
    "immo_3"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the \"first impressions\" chapter, the zip code was removed. Would you think it has value for predicting the cold rent? What would you need to consider?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: answer as comment\n",
    "# the zip code might be useful to have a more clear definition of where the house is located as bigger cities tend to have higher renting prices. As Zip-Codes are not numerical values it would need to be encoded before it can be used as input. Even though zip codes are numbers they have no"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Take a look at the attribute _firingTypes_. According to the dataset author, it describes the main energy source. In the ``.describe()`` table we saw it has 133 unique values. The similar attribute _heatingType_ has only 13 unique values. Output both lists of unique values to compare the two attributes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "print(immo_2['heatingType'])\n",
    "print(immo_2['firingTypes'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We decide to keep the less detailed attribute and keep the dataset simple. What other option do we have? "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: answer as comment\n",
    "# instead of one hot encoding we could think about using embedding. We embed the heating feature using firingTypes and heatingType"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Create a new DataFrame ``immo_4`` without the  _firingTypes_ column."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "immo_4 = immo_3.drop(['firingTypes'], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2: Visual Evaluation of Categorical Features\n",
    "In this section we will take a look at the frequency distribution histograms of all remaining categorical features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "categorical_columns = immo_4.select_dtypes(exclude=np.number).columns\n",
    "\n",
    "n_cols = 5\n",
    "n_rows = int(np.ceil(len(categorical_columns) / n_cols))\n",
    "ax = plt.subplots(nrows=n_rows, ncols=n_cols, squeeze=False)[1].flatten()\n",
    "\n",
    "for i, colname in enumerate(categorical_columns):\n",
    "    immo_4[colname].value_counts().plot(kind=\"barh\", ax=ax[i], title=colname, figsize=(24, 12))\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can see that almost all features now contain only a few unique and meaningful values.\n",
    "\n",
    "Only _regio2_ seems to be too crowded, but that is okay as there still are some values with a lot of occurances.\n",
    "\n",
    "The one-hot encoding itself will be done in a later chapter to keep the number of columns minimal for now.\n",
    "\n",
    "## 4: Analyzing Numerical Data\n",
    "In this section we will take a look at the numeric features. More precisely, we look mainly at outlier detection and handling.\n",
    "\n",
    "### 4.1: Outliers\n",
    "An outlier is an observation that lies an abnormal distance from other values in a given set of data. Outliers can negatively impact the quality of a regression model, which is why it is important to identify them and find a solution to deal with them.\n",
    "\n",
    "#### 4.1.1: ✏️ Outlier Detection\n",
    "Outliers can be spotted visually, e.g. when looking at a histogram or statistically by reviewing the characteristics of each column.\n",
    "First, we will use the ``.describe()`` method again."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# reduce the number of decimals shown in the output (for better readability)\n",
    "pd.set_option(\"display.float_format\", lambda x: f\"{x:.2f}\")\n",
    "\n",
    "immo_4.describe(include=np.number).drop(\"count\").T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Take a look at the table to see if you can find some of the extreme outliers in the data by yourself.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Answer\n",
    "# totalRent, heatingCosts, baseRent, serviceCharge, yearConstructed, noParkSpaces, numberOfFloors, floor, lastRefurbished"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We save the names of all these suspicious columns in a list to further inspect them.\n",
    "Furthermore, we see that the feature _telekomHybridUploadSpeed_ always has the value 10 or is empty, so we delete it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "interesting_columns = [\n",
    "    \"serviceCharge\",\n",
    "    \"totalRent\",\n",
    "    \"yearConstructed\",\n",
    "    \"noParkSpaces\",\n",
    "    \"baseRent\",\n",
    "    \"livingSpace\",\n",
    "    \"noRooms\",\n",
    "    \"numberOfFloors\",\n",
    "    \"heatingCosts\",\n",
    "    \"lastRefurbish\",\n",
    "]\n",
    "\n",
    "immo_5 = immo_4.drop([\"telekomHybridUploadSpeed\"], axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.2: ✏️ Outlier Visualization\n",
    "Outliers can be visualized using histograms and boxplot charts.\n",
    "\n",
    "1. Adapt the visualization code used for categorical data to show histograms of *all* the numerical columns (instead of using ``value_counts()``)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "numeric_columns = immo_5.select_dtypes(include=np.number).columns\n",
    "\n",
    "n_cols = 5\n",
    "n_rows = int(np.ceil(len(numeric_columns) / n_cols))\n",
    "ax = plt.subplots(nrows=n_rows, ncols=n_cols, squeeze=False)[1].flatten()\n",
    "\n",
    "for i, colname in enumerate(numeric_columns):\n",
    "    immo_5[colname].plot(kind=\"hist\", ax=ax[i], title=colname, figsize=(24, 12), bins=30)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Adapt the visualization code to show box plots of *only the interesting* numerical columns "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "numeric_columns = immo_5.select_dtypes(include=np.number).columns\n",
    "\n",
    "n_cols = 5\n",
    "n_rows = int(np.ceil(len(numeric_columns) / n_cols))\n",
    "ax = plt.subplots(nrows=n_rows, ncols=n_cols, squeeze=False)[1].flatten()\n",
    "\n",
    "for i, colname in enumerate(numeric_columns):\n",
    "    immo_5[colname].plot(kind=\"box\", ax=ax[i], title=colname, figsize=(24, 12))\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1.3: Outlier Removal\n",
    "Looking at the plots, we decide that these are too many outliers to adress them individually in this excercise.\n",
    "\n",
    "Instead, we take look at what would happen if we would simply cut off the top and bottom 0.5% of the data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "upper_limits  = immo_5[interesting_columns].quantile(0.005)\n",
    "lower_limits = immo_5[interesting_columns].quantile(0.995)\n",
    "\n",
    "limits = pd.DataFrame({\"lower bound\": upper_limits, \"upper bound\": lower_limits})\n",
    "limits\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a healthier distribution. \n",
    "We therefore create a new DataFrame containing only the rows that fit within these bounds (plus those that are _NaN_, these we will care about later):"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "immo_6 = immo_5.copy()\n",
    "\n",
    "for colname, (lower, upper) in limits.iterrows():\n",
    "    col = immo_5[colname]\n",
    "    # immo_6[colname].loc[:] = np.nan\n",
    "    immo_6[colname] = immo_5[((col <= upper) & (col >= lower)) | col.isna()][colname]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the statistical characteristics and number of _NaN_ cells before and after the conversion to check if everything worked as intended:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nans_5 = immo_5.select_dtypes(include=np.number).isna().sum().rename(\"count_NaN\")\n",
    "nans_6 = immo_6.select_dtypes(include=np.number).isna().sum().rename(\"count_NaN\")\n",
    "desc_5 = immo_5.describe(include=np.number).drop(\"count\").T.join(nans_5)\n",
    "desc_6 = immo_6.describe(include=np.number).drop(\"count\").T.join(nans_6)\n",
    "\n",
    "pd.concat([desc_5, desc_6, desc_6 - desc_5], axis=1, keys=[\"immo_5\", \"immo_6\", \"Δ\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the minimum and maximum values look far more reasonable now. However, the number of _NaN_ fields has increased, as our conversion simply replaced all outlier values with _NaN_.\n",
    "\n",
    "If this is a valid approach depends very much on your task and often times it would make more sense to instead drop all outlier rows. \n",
    "\n",
    "At the very latest, it is now time to take care of the NaN values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Handling Missing Values\n",
    "Let's take a look at how many missing values we have for both kind of data types:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "nans = immo_6.isna().sum().rename(\"count_NaN\").sort_values(ascending=False)\n",
    "dtypes = immo_6.dtypes.rename(\"dtype\")\n",
    "display(nans.to_frame().join(dtypes))\n",
    "print(\"Total Number of NaNs:\", immo_6.isna().sum().sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these columns contain many _NaN_ values, if we ignored all these, there would not be many rows left. \n",
    "\n",
    "We therefore follow this strategy first: \n",
    "\n",
    "- For numeric values, the mean of the remaining values is used for a missing value.\n",
    "\n",
    "- For non-numeric values, the most frequent value (=mode) of the remaining values is used for a missing value.\n",
    "\n",
    "This is a very simple approach, but it obviously will alter the true distribution of our data.\n",
    "\n",
    "### 5.1 ✏️ Replacing Missing Values\n",
    "To fill _NaN_ values, pandas provides the ``.fillna()`` method. It simply replaces all _NaNs_ with a certain value.\n",
    "\n",
    "The machine learning library **scikit-learn** provides a more advanced interface, the [_SimpleImputer_](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html).\n",
    "In contrast, it can replace missing values using a descriptive statistic like mean, median, or the most frequent value along each column.\n",
    "\n",
    "1. Create a _SimpleImputer_ object to replace all _NaN_ values in numerical columns with the mean of the respective column. Apply it to _immo_7_ and replace the respective columns with the new data. Finally, print the total number of NaNs in the new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "immo_7 = immo_6.copy()\n",
    "\n",
    "numeric_columns = immo_7.select_dtypes(include=np.number).columns\n",
    "\n",
    "# TODO: Implement\n",
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "immo_7.loc[:,numeric_columns] = imp_mean.fit_transform(immo_7[numeric_columns])\n",
    "print(\"Total Number of NaNs:\", immo_7.isna().sum().sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a new DataFrame _immo_8_ in which _NaN_ values in all non-numeric columns are replaced with the most frequent value of that column. Again, print the number of NaNs in the new DataFramne (should be 0).  "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "immo_8 = immo_7.copy()\n",
    "\n",
    "imp_freq = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "categorical_columns = immo_8.select_dtypes(exclude=np.number).columns\n",
    "immo_8.loc[:,categorical_columns] = imp_freq.fit_transform(immo_8[categorical_columns])\n",
    "\n",
    "print(\"Total Number of NaNs:\", immo_8.isna().sum().sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Feature Engineering\n",
    "In general, feature engineering is the process of transforming raw data to meaningful features that can be used for machine learning.\n",
    "\n",
    "So far, we have cleaned the raw data to make it easier to decide which features to create.\n",
    "\n",
    "Typical engineered features include[¹](https://en.wikipedia.org/wiki/Feature_engineering):\n",
    "\n",
    "- for numerical data\n",
    "    - Numerical transformations (like taking fractions or scaling)\n",
    "    - Clustering\n",
    "    - Group aggregated values\n",
    "    - Principal component analysis\n",
    "    - Constructed metrics based on proven scientific research in the specific domain.\n",
    "- for categorical data\n",
    "    - Encoded Category encoder (like one-hot or target encoding)\n",
    "\n",
    "### 6.1: Analyzing Correlations\n",
    "We generate scatter plots to identify obvious linear or nonlinear relationships between individual features and the target variable *baseRent*."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "numeric_columns = list(immo_8.select_dtypes(include=np.number).columns)\n",
    "numeric_columns.remove(\"baseRent\")\n",
    "\n",
    "n_cols = 5\n",
    "n_rows = int(np.ceil(len(numeric_columns) / n_cols))\n",
    "ax = plt.subplots(nrows=n_rows, ncols=n_cols, squeeze=False)[1].flatten()\n",
    "\n",
    "for i, colname in enumerate(numeric_columns):\n",
    "    immo_8[[\"baseRent\",colname]].plot.scatter(x=colname, y=\"baseRent\", ax=ax[i], figsize=(24, 14), alpha=0.2)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "✏️ Take a moment to understand the graph. Which correlations do you notice?\n",
    "\n",
    "➡️ The data scatters relatively broadly. As expected, there is correlation between size and rental price.\n",
    "Of course, there also is a correlation between the target feature and closely related features like _baseRent_ - we will handle this in the next chapter. \n",
    "For now, we see no further anomalies.\n",
    "\n",
    "We can also plot the correlation coefficient for all combinations of features (= a correlation matrix) to see which features are similar to each other.\n",
    "For this, we use the **seaborn** library that acts as a wrapper for commonly used matplotlib plots."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(immo_8.corr(numeric_only=True))\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than the already noted correlations to _baseRent_, we only see new correlations in between features and their corresponding _range_ features (e.g. _noRooms_ and _noRoomsRange_). That is acceptable and we will continue without further action.\n",
    "\n",
    "In this example, we will not create any new features other than the one hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ✏️ Applying the One Hot Encoding\n",
    "Next, we transform the categorical columns using OneHot encoding. \n",
    "\n",
    "1. Apply the code mentioned in the One Hot Encoding example from earlier to encode the categorical columns of our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "immo_9 = immo_8.copy()\n",
    "\n",
    "# TODO: Implement\n",
    "categorical_columns = immo_9.select_dtypes(exclude=np.number).columns\n",
    "numerical_columns = immo_9.select_dtypes(include=np.number).columns\n",
    "one_hot = pd.get_dummies(immo_9[categorical_columns])\n",
    "immo_9 = one_hot.merge(immo_9[numerical_columns], left_index=True, right_index=True)\n",
    "immo_9.head(1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the created DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Shape:\", immo_9.shape, \", Columns:\", list(immo_9.columns))\n",
    "immo_9.head(2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altogether, we now have a data matrix with 268850 records (rows) and 515 features (columns).\n",
    "\n",
    "## 7: Handling the Target Column\n",
    "Finally, we will need to extract the target column and remove all columns that might lead the algorithm to overfit because of the high correlation.\n",
    "\n",
    "Keeping these \"easy\" columns in the dataset would almost be cheating ;)\n",
    "\n",
    "Furthermore, it makes sense to remove all rows that do not contain a value for the target column."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "immo_X = immo_9[immo_9[\"baseRent\"].isna() == False]\n",
    "\n",
    "immo_Y = immo_X[\"baseRent\"]\n",
    "immo_X = immo_X.drop([\"baseRent\", \"totalRent\", \"baseRentRange\"], axis=1)\n",
    "print(immo_Y.shape, immo_X.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real world example, you might want to export your final DataFrame now, e.g. to a new csv file.\n",
    "\n",
    "````python\n",
    "immo_X.to_csv('immoscout_prepared_X.csv', index=False)\n",
    "````\n",
    "\n",
    "To keep state consistent between students, we will however not build upon this file now but instead rely on the methods introduced in the following chapter.\n",
    "\n",
    "## 8: Summary of Data Preparation\n",
    "In production, the steps to import and explore the dataset can be summarized in a small number of functions. \n",
    "\n",
    "The goal of this section is to once again read the dataset file and perform only the core preparation steps we identified so far:\n",
    "- delete irrelevant and target-related columns\n",
    "- delete outliers\n",
    "- impute NaN values\n",
    "- one-hot encode categorical features\n",
    "- remove rows without label\n",
    "- extract target column\n",
    "\n",
    "We define re-usable methods (for this specific dataset) and then call them subsequently."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def drop_columns(df):\n",
    "    return df.drop(\n",
    "        [\n",
    "            \"scoutId\",\n",
    "            \"houseNumber\",\n",
    "            \"geo_bln\",\n",
    "            \"geo_krs\",\n",
    "            \"geo_plz\",\n",
    "            \"date\",\n",
    "            \"street\",\n",
    "            \"streetPlain\",\n",
    "            \"description\",\n",
    "            \"facilities\",\n",
    "            \"regio3\",\n",
    "            \"firingTypes\",\n",
    "            \"telekomHybridUploadSpeed\",\n",
    "            \"totalRent\",\n",
    "            \"baseRentRange\",\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "\n",
    "def remove_outliers(df):\n",
    "    dfc = df.copy()\n",
    "    columns_with_outliers = [\n",
    "        \"serviceCharge\",\n",
    "        \"yearConstructed\",\n",
    "        \"noParkSpaces\",\n",
    "        \"baseRent\",\n",
    "        \"livingSpace\",\n",
    "        \"noRooms\",\n",
    "        \"numberOfFloors\",\n",
    "        \"heatingCosts\",\n",
    "        \"lastRefurbish\",\n",
    "    ]\n",
    "\n",
    "    upper_limits = dfc[columns_with_outliers].quantile(0.995)\n",
    "    lower_limits = dfc[columns_with_outliers].quantile(0.005)\n",
    "    for colname in columns_with_outliers:\n",
    "        col = dfc[colname]\n",
    "        dfc = dfc[\n",
    "            ((col <= upper_limits[colname]) & (col >= lower_limits[colname]))\n",
    "            | col.isna()\n",
    "        ]\n",
    "    return dfc\n",
    "\n",
    "\n",
    "def remove_rows_with_NaN_target(df):\n",
    "    return df[df[\"baseRent\"].isna() == False]\n",
    "\n",
    "\n",
    "def impute_NaNs(df):\n",
    "    dfc = df.copy()\n",
    "    categorical_columns = dfc.select_dtypes(exclude=np.number).columns\n",
    "    imp_freq = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")\n",
    "    dfc.loc[:, categorical_columns] = imp_freq.fit_transform(dfc[categorical_columns])\n",
    "\n",
    "    numeric_columns = dfc.select_dtypes(include=np.number).columns\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\n",
    "    dfc.loc[:, numeric_columns] = imp_mean.fit_transform(dfc[numeric_columns])\n",
    "    return dfc\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling these methods now to prepare data based on the raw original dataset is easy and produces very well readable code:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "immo_p = drop_columns(immo)\n",
    "immo_p = remove_outliers(immo_p)\n",
    "immo_p = remove_rows_with_NaN_target(immo_p)\n",
    "immo_p = impute_NaNs(immo_p)\n",
    "immo_p = pd.get_dummies(immo_p)\n",
    "immo_p_Y = immo_p.pop(\"baseRent\")\n",
    "\n",
    "immo_p.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9: Splitting and Scaling\n",
    "In addition to the steps we learned in the last notebook, certain methods can be useful depending on your chosen model architecture and error metric.\n",
    "\n",
    "### 9.1: Split Training and Test Dataset\n",
    "Splitting the data is important for every supervised learning task. Seperating training and test data ensures that the model is not tested against data it has already seen during the training phase.\n",
    "\n",
    "In the following example, the ``train_test_split()`` method from scikit-learn is used to create a test-split of 20% the size of the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    immo_p, immo_p_Y, test_size=0.2, random_state=42\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of scikit-learn is Pandas-compatible and will accept and return DataFrames if feasible. The same method can be used to split numpy arrays, in which case it would return a numpy array instead.\n",
    "\n",
    "### 9.2: Scaling\n",
    "Feature scaling, sometimes called normalization, is a technique used to standardize the independent features of a dataset to a fixed range. \n",
    "\n",
    "If values or units are of different magnitudes, some ML methods tend to overfit on high values while disregarding smaller values.\n",
    "\n",
    "Scikit-Learn provides multiple [methods for scaling data](https://scikit-learn.org/stable/modules/preprocessing.html).\n",
    "\n",
    "In this example, we will use the ``MinMaxScaler`` to scale each feature individually to the range $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10: Linear Regression\n",
    "As introduced in the lecture, a linear regression tries to fit a linear function of the form $ f(X) = w_0 + \\sum^{|X|}_{j=1} (w_j * x_j) $ by learning weights $W$ that minimize the error $d$ between $f(X)$ and the actual label $y$ for a given feature-vector $X$. In our case, $X$ represents a row (data sample) of the DataFrame, and $x_j$ therefore is a certain features value for this row.\n",
    "\n",
    "The error $d$ is measured using a loss function, often times the least squares approach is used with $ d(X_i, y_i) = (y_i - f(X_i) )^2 $, with the $i$ indicating that we can calculate the distance for different rows (feature-vectors $X_i$), each having it's own label $y_i$. \n",
    "\n",
    "In Training, we calculate the total loss by iterating over all rows: $ L(W) = \\frac{\\sum^N_{i=1}(d_i)}{N}$. \n",
    "\n",
    "Using scikit-learn, this math is hidden from the programmer. However, it is stil important to understand these basics to differentiate and choose the right architecture for a given problem.\n",
    "\n",
    "### 10.1: Train\n",
    "During training, the weights are learned and the function is _fitted_. Hence the name of the scikit-learn function:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_lr = linear_model.LinearRegression()\n",
    "model_lr.fit(X_train_scaled, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has now learned several weights (coefficients in the function) that can be accessed using a attribute:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model_lr.coef_[0:10]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2: Test\n",
    "We can now apply the fitted function $f(X)$ for various $X$. In this case, we test both the test and the training set, even though only the test involving the test data has significance for the real-world accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "y_test_pred = model_lr.predict(X_test_scaled)\n",
    "y_train_pred = model_lr.predict(X_train_scaled)\n",
    "\n",
    "print(y_test_pred[0:5])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3: Alternative Approach: Scikit-Learn Pipelines\n",
    "A common mistake is to apply scaling methods to the entire data instead of train and test splits individually, leaking information from the test set to the training set.\n",
    "\n",
    "Additionally, it is easy to forget which part of the data has to be scaled for which steps of the machine learning process.\n",
    "  \n",
    "It therefore is recommended to call the above methods within a [Pipeline](https://scikit-learn.org/stable/modules/compose.html#pipeline) in order to prevent most risks of data leaking: \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "pipe_lr = make_pipeline(MinMaxScaler(), linear_model.LinearRegression())\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_pipe = pipe_lr.predict(X_test)\n",
    "y_train_pred_pipe = pipe_lr.predict(X_train)\n",
    "\n",
    "print(y_test_pred_pipe[0:5])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All scaling will automatically be done if calling the methods of the pipeline.\n",
    "\n",
    "### 10.4: Evaluation\n",
    "\n",
    "We will evaluate the trained models using the following metrics:\n",
    "- R²\n",
    "- Mean Squared Error (MSE) \n",
    "- Mean Absolute Error (MAE)\n",
    "\n",
    "They are all available in the sklearn framework and will be applied for the train and test splits separately."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def print_evaluation(pipeline_or_model, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred, feature_names):\n",
    "    train = pipeline_or_model.score(X_train, y_train)\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "\n",
    "    r2_test = pipeline_or_model.score(X_test, y_test)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "    print(\n",
    "        f\"{pipeline_or_model} Evaluation:\\n\"\n",
    "        f\"{'':6} {'R²':>10} | {'MSE':>14} | {'MAE':>10} | {'rows':>8} | {'columns':>8}\\n\"\n",
    "        f\"{'Train':6} {train:10.2f} | {mse_train:14.2f} | {mae_train:10.2f} | {X_train.shape[0]:8} | {X_train.shape[1]:8}\\n\"\n",
    "        f\"{'Test':6} {r2_test:10.2f} | {mse_test:14.2f} | {mae_test:10.2f} | {X_test.shape[0]:8} | {X_test.shape[1]:8}\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "print_evaluation(model_lr, X_train_scaled, X_test_scaled, y_train, y_test, y_train_pred, y_test_pred, immo_p.columns)\n",
    "print_evaluation(pipe_lr, X_train, X_test, y_train, y_test, y_train_pred_pipe, y_test_pred_pipe, immo_p.columns)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using the pipeline produces equal results.\n",
    "\n",
    "### 10.5: Visualization\n",
    "The result of this 512 dimensional multiple linear regression is not easy to visualize in 2D or 3D graphs.\n",
    "\n",
    "To still get some visual insight to our accuracy, we create a scatterplot showing samples of the test set in two dimensions: \n",
    "The size of the apartment (X-axis) and the cold rent (Y-axis), both as groundtruth and as prediction. \n",
    "\n",
    "For further insight, feel free to create more scatterplots."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.plot(X_test[\"livingSpace\"], y_test, \"bo\", alpha=0.1, label=\"ground truth\")\n",
    "plt.plot(X_test[\"livingSpace\"], y_test_pred, \"ro\", alpha=0.1, label=\"predicted\")\n",
    "plt.gca().update(dict(title=\"Immo Prediction\", xlabel=\"appartment size\", ylabel=\"cold rent\"))\n",
    "plt.legend(title=\"cold rent\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that they roughly overlap, however the points are far from matching perfectly.\n",
    "\n",
    "## 11: ✏️ Linear Regression Excercises\n",
    "1. Copy and adapt the code from above: How does the prediction quality change when outliers are not removed?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "immo_p2 = drop_columns(immo)\n",
    "immo_p2 = remove_rows_with_NaN_target(immo_p2)\n",
    "immo_p2 = impute_NaNs(immo_p2)\n",
    "immo_p2 = pd.get_dummies(immo_p2)\n",
    "immo_p2_Y = immo_p2.pop(\"baseRent\")\n",
    "\n",
    "# splitting data\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
    "    immo_p2, immo_p2_Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "model_lr = linear_model.LinearRegression()\n",
    "model_lr.fit(X2_train, y2_train)\n",
    "\n",
    "y2_test_pred = model_lr.predict(X2_test)\n",
    "y2_train_pred = model_lr.predict(X2_train)\n",
    "\n",
    "# def print_evaluation(pipeline_or_model, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred, feature_names):\n",
    "print(\"Unscaled result:\")\n",
    "print_evaluation(model_lr, X2_train, X2_test, y2_train, y2_test,y2_train_pred, y2_test_pred, immo_p2.columns)\n",
    "print(\"Scaled result:\")\n",
    "print_evaluation(pipe_lr, X_train, X_test, y_train, y_test, y_train_pred_pipe, y_test_pred_pipe, immo_p.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a region-specific model, trained on only a certain geographical region (try it once with Baden-Württemberg and another time with just Karlsruhe)\n",
    "   Which model does show an overfitting problem: the model of Baden-Württemberg or the model of Karlsruhe? "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Baden-Württemberg dataset immo_p3 and the target is immo_p3_Y\n",
    "immo_p3 = drop_columns(immo)\n",
    "# TODO: Implement\n",
    "immo_p3 = immo_p3[immo_p3['regio1'].str.contains(\"Baden_Württemberg\")]\n",
    "immo_p3 = remove_rows_with_NaN_target(immo_p3)\n",
    "immo_p3 = remove_outliers(immo_p3)\n",
    "immo_p3 = impute_NaNs(immo_p3)\n",
    "immo_p3 = pd.get_dummies(immo_p3)\n",
    "immo_p3_Y = immo_p3.pop(\"baseRent\")\n",
    "\n",
    "immo_p3.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# data splitting\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(\n",
    "    immo_p3, immo_p3_Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model and its evaluation\n",
    "## TODO: Implement\n",
    "pipe_lr3 = make_pipeline(MinMaxScaler(), linear_model.LinearRegression())\n",
    "pipe_lr3.fit(X_train_3, y_train_3)\n",
    "\n",
    "y_test_pred_pipe_3 = pipe_lr3.predict(X_test_3)\n",
    "y_train_pred_pipe_3 = pipe_lr3.predict(X_train_3)\n",
    "\n",
    "# def print_evaluation(pipeline_or_model, X_train, X_test, y_train, y_test, y_train_pred, y_test_pred, feature_names):\n",
    "print_evaluation(pipe_lr3, X_train_3, X_test_3, y_train_3, y_test_3, y_train_pred_pipe_3, y_test_pred_pipe_3, immo_p3.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.plot(X_test_3[\"livingSpace\"], y_test_3, \"bo\", alpha=0.3, label=\"ground truth\")\n",
    "plt.plot(X_test_3[\"livingSpace\"], y_test_pred_pipe_3, \"ro\", alpha=0.3, label=\"predicted\")\n",
    "plt.gca().update(dict(title=\"Immo Prediction\", xlabel=\"appartment size\", ylabel=\"cold rent\"))\n",
    "plt.legend(title=\"cold rent\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Karlsruhe dataset immp_p4 and the target is immo_p4_Y\n",
    "immo_karlsruhe = immo[immo['geo_krs'].str.contains(\"Karlsruhe\")]\n",
    "immo_p4 = drop_columns(immo_karlsruhe)\n",
    "# TODO: Implement\n",
    "immo_p4 = remove_rows_with_NaN_target(immo_p4)\n",
    "immo_p4 = remove_outliers(immo_p4)\n",
    "immo_p4 = impute_NaNs(immo_p4)\n",
    "immo_p4 = pd.get_dummies(immo_p4)\n",
    "immo_p4_Y = immo_p4.pop(\"baseRent\")\n",
    "immo_karlsruhe.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# data splitting\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(\n",
    "    immo_p4, immo_p4_Y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Model and its evaluation\n",
    "## TODO: Implement\n",
    "pipe_lr4 = make_pipeline(MinMaxScaler(), linear_model.LinearRegression())\n",
    "pipe_lr4.fit(X_train_4, y_train_4)\n",
    "\n",
    "y_test_pred_pipe_4 = pipe_lr4.predict(X_test_4)\n",
    "y_train_pred_pipe_4 = pipe_lr4.predict(X_train_4)\n",
    "\n",
    "print_evaluation(pipe_lr4, X_train_4, X_test_4, y_train_4, y_test_4,y_train_pred_pipe_4, y_test_pred_pipe_4, immo_p4.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(18, 5))\n",
    "plt.plot(X_test_4[\"livingSpace\"], y_test_4, \"bo\", alpha=0.3, label=\"ground truth\")\n",
    "plt.plot(X_test_4[\"livingSpace\"], y_test_pred_pipe_4, \"ro\", alpha=0.3, label=\"predicted\")\n",
    "plt.gca().update(dict(title=\"Immo Prediction\", xlabel=\"appartment size\", ylabel=\"cold rent\"))\n",
    "plt.legend(title=\"cold rent\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Predict the cold rent for your own appartment. Think about how the feature vector would look like for your appartment, create a DataFrame with one row and apply the best performing of the previously learned models using ``.predict()``. Take a look at ``column_descriptions`` if you are unsure of what a certain column means. 💡 This excercise is not as easy as it sounds, as you need to perform the preprocessing steps while making sure NaN values are correctly filled and one-hot-encoding works as intended to get exactly the same DataFrame structure used during fitting the model!\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T15:04:32.053122Z",
     "start_time": "2025-04-10T15:04:32.035115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_apartment_data = {\n",
    "    'regio1': 'Baden_Württemberg',\n",
    "    'serviceCharge': 20,\n",
    "    'newlyConst': False,\n",
    "    'balcony': False,\n",
    "    'telekomUploadSpeed': 1000.0,\n",
    "    'noParkSpaces': 0,\n",
    "    'hasKitchen': True,\n",
    "    'cellar': False,\n",
    "    'heatingType': 'gas_heating',\n",
    "    'yearConstructedRange': 1,\n",
    "    'livingSpace': 12,\n",
    "    'condition': 'well_kept',\n",
    "    'interiorQual': 'normal',\n",
    "    'lift': False,\n",
    "    'noRooms': 1,\n",
    "    'floor': 2,\n",
    "    'numberOfFloors': 4,\n",
    "    'garden': False,\n",
    "    'noRoomsRange': 1,\n",
    "    'livingSpaceRange': 1,\n",
    "    'telekomTvOffer': 'NONE',\n",
    "    'regio2': 'Karlsruhe',\n",
    "    'typeOfFlat': 'apartment',\n",
    "    'description': 'EE appartment with a nice view',\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([my_apartment_data])"
   ],
   "outputs": [],
   "execution_count": 296
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T15:07:26.330041Z",
     "start_time": "2025-04-10T15:07:23.524361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_apartment_data = pd.concat([immo, df], ignore_index=True)\n",
    "\n",
    "my_apartment_data = drop_columns(my_apartment_data)\n",
    "immo_temp = drop_columns(immo)\n",
    "my_apartment_data = impute_NaNs(my_apartment_data)\n",
    "\n",
    "categorical_columns = immo_temp.select_dtypes(exclude=np.number).columns\n",
    "numerical_columns = immo_temp.select_dtypes(include=np.number).columns\n",
    "one_hot = pd.get_dummies(my_apartment_data[categorical_columns])\n",
    "my_apartment_data = my_apartment_data[numerical_columns].merge(one_hot, left_index=True, right_index=True)\n",
    "\n",
    "my_apartment_data = pd.get_dummies(my_apartment_data)\n",
    "my_apartment_data = my_apartment_data.reindex(immo_p.columns, axis=1)"
   ],
   "outputs": [],
   "execution_count": 305
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T15:08:51.983680Z",
     "start_time": "2025-04-10T15:08:51.968162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "my_apartment_data_X = my_apartment_data.tail(1)\n",
    "\n",
    "pipe_lr.predict(my_apartment_data_X)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([326.40625])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 307
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12: Regularization \n",
    "\n",
    "**Regularization** in ML is a term that describes methods to simplify a regression problem and **reduce the generalization error** (error on test data, not training data). It can be separated into two kinds:\n",
    "\n",
    "- In **Explicit regularization** one explicitly adds a bias (penalty/regularization) term to the optimization problem.\n",
    "    - Ridge Regression (L2 regularization): the cost function is altered by adding the _Ridge Regression penalty_ bias term (squared magnitude of the coefficient) to it.\n",
    "    - Lasso Regression (L1 regularization): \"Least Absolute and Selection Operator\": similar to the Ridge Regression except that the penalty term contains only the **absolute** weights instead of a square of weights\n",
    "        - Since it takes absolute values, hence, it can shrink the slope to 0, whereas Ridge Regression can only shrink it near 0.\n",
    "- **Implicit regularization** is all other forms of regularization. This includes, for example, early stopping, using a robust loss function, and discarding outliers. Implicit regularization is essentially ubiquitous in modern machine learning approaches, including stochastic gradient descent for training deep neural networks, and ensemble methods (such as random forests and gradient boosted trees).\n",
    "\n",
    "In this chapter we utilize the L1 & L2 regularization.\n",
    "\n",
    "### 12.1: ✏️ Regression with Regularization\n",
    "Using the L1 and L2 regularization is simple when using Scikit-Learn. \n",
    "\n",
    "1. Read [the documentation](https://scikit-learn.org/stable/modules/linear_model.html) to find the right methods for L1 and L2 regularization. Train both models on the geographical region from task 11.2 that showed an overfitting problem (Baden-Württemberg \"immo_p3\" or Karlsruhe \"immo_p4\") and print the evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "from sklearn.linear_model import Ridge\n",
    "# Ridge is L2 regularization\n",
    "# will take in its fit method arrays X, y and will store the w coefficients of the linear model in its coef_ member\n",
    "model_3 = Ridge()\n",
    "pipe_lr3 = make_pipeline(MinMaxScaler(), model_3)\n",
    "pipe_lr3.fit(X_train_3, y_train_3)\n",
    "\n",
    "y_test_pred_pipe_3 = pipe_lr3.predict(X_test_3)\n",
    "y_train_pred_pipe_3 = pipe_lr3.predict(X_train_3)\n",
    "\n",
    "print_evaluation(pipe_lr3, X_train_3, X_test_3, y_train_3, y_test_3, y_train_pred_pipe_3, y_test_pred_pipe_3, immo_p3.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_4 = linear_model.Lasso()\n",
    "# The Lasso is a linear model that estimates sparse coefficients.\n",
    "# The lasso estimate thus solves the minimization of the least-squares penalty with a||w|| added, where a is a constant and ||w|| is the -norm of the coefficient vector.\n",
    "\n",
    "pipe_lr = make_pipeline(MinMaxScaler(), model_4)\n",
    "pipe_lr.fit(X_train_4, y_train_4)\n",
    "y_test_pred_pipe_4 = pipe_lr.predict(X_test_4)\n",
    "y_train_pred_pipe_4 = pipe_lr.predict(X_train_4)\n",
    "print_evaluation(pipe_lr, X_train_4, X_test_4, y_train_4, y_test_4,y_train_pred_pipe_4, y_test_pred_pipe_4, immo_p4.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Both methods accept a parameter ``alpha``. What can this parameter be used for? Apply the same code from the previous excercise for another alpha value."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: Answer and Implement\n",
    "# The alpha parameter controls the degree of sparsity of the estimated coefficients."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.3: Hyperparameter Optimization\n",
    "It now has become apparent that the parameter ``alpha`` can influence the quality of our L1 and L2 models.\n",
    "However, this is just one example. Almost every ML model can be parameterized and finding the best parameters is a challenging task.\n",
    "\n",
    "Systematic approaches to finding ideal parameters are called [hyperparameter optimization](https://en.wikipedia.org/wiki/Hyperparameter_optimization).\n",
    "They are based on the idea of systematically trying out different values for each parameters and evaluating which of them work best. \n",
    "\n",
    "A traditional but still popular hyperparameter optimization approach is called **grid search**, which is also provided as a [scikit-learn module](https://scikit-learn.org/stable/modules/grid_search.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:03.871300Z",
     "start_time": "2025-04-10T14:55:56.998180Z"
    }
   },
   "source": [
    "# The used dataset here is immo_p4 you can try another dataset \n",
    "\n",
    "grid_clf = GridSearchCV(\n",
    "    linear_model.Ridge(),\n",
    "    {\"alpha\": [1e-3, 1e-2, 1e-1, 1, 10, 100]},\n",
    "    n_jobs=-1,\n",
    "    return_train_score=True,\n",
    "    verbose=5,\n",
    ")\n",
    "\n",
    "pipe_grid = make_pipeline(MinMaxScaler(), grid_clf)\n",
    "pipe_grid.fit(X_train_4, y_train_4)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
       "                ('gridsearchcv',\n",
       "                 GridSearchCV(estimator=Ridge(), n_jobs=-1,\n",
       "                              param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10,\n",
       "                                                    100]},\n",
       "                              return_train_score=True, verbose=5))])"
      ],
      "text/html": [
       "<style>#sk-container-id-4 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-4 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-4 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-4 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-4 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-4 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-4 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-4 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-4 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-4 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-4 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;minmaxscaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;gridsearchcv&#x27;,\n",
       "                 GridSearchCV(estimator=Ridge(), n_jobs=-1,\n",
       "                              param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10,\n",
       "                                                    100]},\n",
       "                              return_train_score=True, verbose=5))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;Pipeline<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.pipeline.Pipeline.html\">?<span>Documentation for Pipeline</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;minmaxscaler&#x27;, MinMaxScaler()),\n",
       "                (&#x27;gridsearchcv&#x27;,\n",
       "                 GridSearchCV(estimator=Ridge(), n_jobs=-1,\n",
       "                              param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10,\n",
       "                                                    100]},\n",
       "                              return_train_score=True, verbose=5))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;MinMaxScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.MinMaxScaler.html\">?<span>Documentation for MinMaxScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>MinMaxScaler()</pre></div> </div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;gridsearchcv: GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for gridsearchcv: GridSearchCV</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(estimator=Ridge(), n_jobs=-1,\n",
       "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "             return_train_score=True, verbose=5)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Ridge</label><div class=\"sk-toggleable__content fitted\"><pre>Ridge(alpha=1)</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;Ridge<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.Ridge.html\">?<span>Documentation for Ridge</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>Ridge(alpha=1)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 287
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this optimization can be accessed as an attribute of the gridsearch object. \n",
    "We quickly convert the returned dictionary to a Pandas DataFrame for easier reading."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:56:52.496060Z",
     "start_time": "2025-04-10T14:56:52.444483Z"
    }
   },
   "source": [
    "pd.DataFrame(grid_clf.cv_results_).set_index(\"params\").sort_values(\"mean_test_score\", ascending=False)\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                  mean_fit_time  std_fit_time  mean_score_time  \\\n",
       "params                                                           \n",
       "{'alpha': 1}               0.02          0.04             0.00   \n",
       "{'alpha': 0.1}             0.11          0.00             0.00   \n",
       "{'alpha': 0.01}            0.10          0.00             0.00   \n",
       "{'alpha': 0.001}           0.10          0.01             0.00   \n",
       "{'alpha': 10}              0.01          0.00             0.00   \n",
       "{'alpha': 100}             0.00          0.00             0.00   \n",
       "\n",
       "                  std_score_time  param_alpha  split0_test_score  \\\n",
       "params                                                             \n",
       "{'alpha': 1}                0.00         1.00               0.75   \n",
       "{'alpha': 0.1}              0.00         0.10               0.75   \n",
       "{'alpha': 0.01}             0.00         0.01               0.75   \n",
       "{'alpha': 0.001}            0.00         0.00               0.75   \n",
       "{'alpha': 10}               0.00        10.00               0.75   \n",
       "{'alpha': 100}              0.00       100.00               0.60   \n",
       "\n",
       "                  split1_test_score  split2_test_score  split3_test_score  \\\n",
       "params                                                                      \n",
       "{'alpha': 1}                   0.77               0.81               0.77   \n",
       "{'alpha': 0.1}                 0.78               0.81               0.77   \n",
       "{'alpha': 0.01}                0.78               0.81               0.78   \n",
       "{'alpha': 0.001}               0.78               0.81               0.78   \n",
       "{'alpha': 10}                  0.75               0.79               0.77   \n",
       "{'alpha': 100}                 0.59               0.64               0.64   \n",
       "\n",
       "                  split4_test_score  mean_test_score  std_test_score  \\\n",
       "params                                                                 \n",
       "{'alpha': 1}                   0.74             0.77            0.02   \n",
       "{'alpha': 0.1}                 0.72             0.77            0.03   \n",
       "{'alpha': 0.01}                0.71             0.76            0.04   \n",
       "{'alpha': 0.001}               0.70             0.76            0.04   \n",
       "{'alpha': 10}                  0.75             0.76            0.02   \n",
       "{'alpha': 100}                 0.60             0.61            0.02   \n",
       "\n",
       "                  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "params                                                                      \n",
       "{'alpha': 1}                    1                0.82                0.81   \n",
       "{'alpha': 0.1}                  2                0.83                0.82   \n",
       "{'alpha': 0.01}                 3                0.83                0.82   \n",
       "{'alpha': 0.001}                4                0.83                0.82   \n",
       "{'alpha': 10}                   5                0.80                0.79   \n",
       "{'alpha': 100}                  6                0.64                0.64   \n",
       "\n",
       "                  split2_train_score  split3_train_score  split4_train_score  \\\n",
       "params                                                                         \n",
       "{'alpha': 1}                    0.81                0.82                0.82   \n",
       "{'alpha': 0.1}                  0.81                0.82                0.83   \n",
       "{'alpha': 0.01}                 0.81                0.82                0.83   \n",
       "{'alpha': 0.001}                0.81                0.82                0.83   \n",
       "{'alpha': 10}                   0.78                0.79                0.80   \n",
       "{'alpha': 100}                  0.63                0.63                0.64   \n",
       "\n",
       "                  mean_train_score  std_train_score  \n",
       "params                                               \n",
       "{'alpha': 1}                  0.82             0.01  \n",
       "{'alpha': 0.1}                0.82             0.01  \n",
       "{'alpha': 0.01}               0.82             0.01  \n",
       "{'alpha': 0.001}              0.82             0.01  \n",
       "{'alpha': 10}                 0.79             0.01  \n",
       "{'alpha': 100}                0.63             0.01  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>params</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>{'alpha': 1}</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.02</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'alpha': 0.1}</th>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.03</td>\n",
       "      <td>2</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'alpha': 0.01}</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>3</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'alpha': 0.001}</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "      <td>4</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'alpha': 10}</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.02</td>\n",
       "      <td>5</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>{'alpha': 100}</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.02</td>\n",
       "      <td>6</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 288
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further use of the pipeline object will automatically use the best estimator, which is refitted with the best found parameters on the whole training dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:57:10.282417Z",
     "start_time": "2025-04-10T14:57:10.224524Z"
    }
   },
   "source": [
    "y_test_pred_grid = pipe_grid.predict(X_test_4)\n",
    "y_train_pred_grid = pipe_grid.predict(X_train_4)\n",
    "\n",
    "print_evaluation(\n",
    "    pipe_grid,\n",
    "    X_train_4,\n",
    "    X_test_4,\n",
    "    y_train_4,\n",
    "    y_test_4,\n",
    "    y_train_pred_grid,\n",
    "    y_test_pred_grid,\n",
    "    immo_p4.columns,\n",
    ")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('minmaxscaler', MinMaxScaler()),\n",
      "                ('gridsearchcv',\n",
      "                 GridSearchCV(estimator=Ridge(), n_jobs=-1,\n",
      "                              param_grid={'alpha': [0.001, 0.01, 0.1, 1, 10,\n",
      "                                                    100]},\n",
      "                              return_train_score=True, verbose=5))]) Evaluation:\n",
      "               R² |            MSE |        MAE |     rows |  columns\n",
      "Train        0.81 |       20258.88 |     105.76 |      768 |       79\n",
      "Test         0.76 |       29274.80 |     130.47 |      193 |       79\n",
      "\n"
     ]
    }
   ],
   "execution_count": 289
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12.4: K-Fold Cross-Validation\n",
    "If you paid close attention to the GridSearch documentation or the returned result table, you might have already seen that the hyperparameter optimizer uses a technique called **K-Fold Cross-Validation**, whose basic idea is as follows: \n",
    "\n",
    "1. The total dataset $T$ is randomly divided into $k$ equally sized subsets (splits/folds) $T_1 \\dots T_k$.\n",
    "2. For each iteration $i_1 \\dots i_k$ a different subset $T_i$ is used as the test dataset and the remaining data $(T \\setminus T_i)$ is used as the training dataset.\n",
    "3. The mean value of the accuracies of the individual iterations is used as the overall result of the cross-validation.\n",
    "\n",
    "This way, no explicit splitting of training and testing set is necessary, instead, a variety of splits will be automatically done and evaluated.\n",
    "This has the advantage that meaningless accuracies of bad splits (e.g. when a certain class is only represented in test split but never in the training split) are averaged out and the mean_accuracy is more representative for the full dataset (and not just the current split).\n",
    "\n",
    "In theory, we can run cross-validation on the full dataset (``immo_1``) and not just on the training split we defined earlier (``X_train`` and ``y_train``), however we would then have no more data to predict (or do a final evaluation on, independendent of all regularization techniques etc.). Depending on your task, you might also not have access to the test set labels but instead can only submit your predictions to a third party that can calculate you a score for it (Common for data-science challenges like on [Kaggle](https://www.kaggle.com/) or the [Data-Mining-Cup](https://www.data-mining-cup.com/)).\n",
    "\n",
    "In the previous example using GridSearch, a 5-fold cross-validation has automatically been used for each value of alpha. Scikit-Learn provides integrated models as well, e.g. [RidgeCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html).\n",
    "\n",
    "Of course, it is also possible to manually call the helper methods for cross-valdiation, and a simple grid search can also be done manually:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T14:59:48.174090Z",
     "start_time": "2025-04-10T14:59:46.098246Z"
    }
   },
   "source": [
    "# The used dataset here is immo_p4 you can try another dataset \n",
    "\n",
    "results_kfcv = {}\n",
    "\n",
    "# Loop all alphas we want to try\n",
    "for alpha in [1e-3, 1e-2, 1e-1, 1, 10, 100]:\n",
    "    scores = []\n",
    "    # Loop over 5 different Train/Test-Splits\n",
    "    splits = KFold(n_splits=5, shuffle=True, random_state=42).split(X_train_4)\n",
    "    for train_index, test_index in splits:\n",
    "        scores.append(\n",
    "            make_pipeline(MinMaxScaler(), linear_model.Ridge(alpha=alpha))\n",
    "            .fit(X_train.iloc[train_index], y_train.iloc[train_index])\n",
    "            .score(X_train.iloc[test_index], y_train.iloc[test_index])\n",
    "        )\n",
    "    # Save mean and standard deviation over the k=5 runs for every alpha\n",
    "    results_kfcv[alpha] = {\n",
    "        # \"scores\": scores,\n",
    "        \"mean_score\": np.mean(scores),\n",
    "        \"stddev_score\": np.std(scores),\n",
    "    }\n",
    "\n",
    "pd.DataFrame(results_kfcv).T.sort_values(\"mean_score\", ascending=False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        mean_score  stddev_score\n",
       "1.00          0.80          0.04\n",
       "0.10          0.78          0.04\n",
       "10.00         0.77          0.04\n",
       "0.01          0.76          0.04\n",
       "0.00          0.76          0.04\n",
       "100.00        0.58          0.05"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_score</th>\n",
       "      <th>stddev_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.78</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10.00</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100.00</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 291
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing alpha can again be easily retreived from these results. It can then be used as a parameter for training a new scikit-learn model with the whole training dataset.\n",
    "\n",
    "### 12.5: ✏️ Advanced Regression Exercise:\n",
    "1. Evaluate which out of three alpha values that you define work best when fitting a _Lasso_ regression model on the geographical region from task 11.2 that showed an overfitting problem (Baden-Württemberg \"immo_p3\" or Karlsruhe \"immo_p4\") using a 3-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T15:11:19.996190Z",
     "start_time": "2025-04-10T15:11:10.858699Z"
    }
   },
   "source": [
    "# TODO: Implement\n",
    "# The used dataset here is immo_p4 you can try another dataset\n",
    "\n",
    "results_kfcv = {}\n",
    "\n",
    "# Loop all alphas we want to try\n",
    "for alpha in [1e-2, 1e-1, 1]:\n",
    "    scores = []\n",
    "    # Loop over 5 different Train/Test-Splits\n",
    "    splits = KFold(n_splits=3, shuffle=True, random_state=42).split(X_train_3)\n",
    "    for train_index, test_index in splits:\n",
    "        scores.append(\n",
    "            make_pipeline(MinMaxScaler(), linear_model.Lasso(alpha=alpha))\n",
    "            .fit(X_train.iloc[train_index], y_train.iloc[train_index])\n",
    "            .score(X_train.iloc[test_index], y_train.iloc[test_index])\n",
    "        )\n",
    "    # Save mean and standard deviation over the k=5 runs for every alpha\n",
    "    results_kfcv[alpha] = {\n",
    "        # \"scores\": scores,\n",
    "        \"mean_score\": np.mean(scores),\n",
    "        \"stddev_score\": np.std(scores),\n",
    "    }\n",
    "\n",
    "pd.DataFrame(results_kfcv).T.sort_values(\"mean_score\", ascending=False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      mean_score  stddev_score\n",
       "0.01        0.83          0.00\n",
       "0.10        0.83          0.01\n",
       "1.00        0.79          0.00"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_score</th>\n",
       "      <th>stddev_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.10</th>\n",
       "      <td>0.83</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.00</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 308
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T15:13:24.339823Z",
     "start_time": "2025-04-10T15:13:23.952346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TODO: Implement\n",
    "# The used dataset here is immo_p4 you can try another dataset\n",
    "\n",
    "results_kfcv = {}\n",
    "\n",
    "# Loop all alphas we want to try\n",
    "for alpha in [1, 10, 100]:\n",
    "    scores = []\n",
    "    # Loop over 5 different Train/Test-Splits\n",
    "    splits = KFold(n_splits=3, shuffle=True, random_state=42).split(X_train_4)\n",
    "    for train_index, test_index in splits:\n",
    "        scores.append(\n",
    "            make_pipeline(MinMaxScaler(), linear_model.Lasso(alpha=alpha))\n",
    "            .fit(X_train.iloc[train_index], y_train.iloc[train_index])\n",
    "            .score(X_train.iloc[test_index], y_train.iloc[test_index])\n",
    "        )\n",
    "    # Save mean and standard deviation over the k=5 runs for every alpha\n",
    "    results_kfcv[alpha] = {\n",
    "        # \"scores\": scores,\n",
    "        \"mean_score\": np.mean(scores),\n",
    "        \"stddev_score\": np.std(scores),\n",
    "    }\n",
    "\n",
    "pd.DataFrame(results_kfcv).T.sort_values(\"mean_score\", ascending=False)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     mean_score  stddev_score\n",
       "1          0.79          0.02\n",
       "10         0.64          0.00\n",
       "100       -0.00          0.00"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_score</th>\n",
       "      <th>stddev_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 310
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbc3c3d932324566a9bf4b4a52ddf64063695fc3adbf25b3fda92572428493bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
