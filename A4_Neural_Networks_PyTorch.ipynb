{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "207ad027-6513-4f7b-a8a9-64da5fe9bbce",
   "metadata": {},
   "source": [
    "# [A4] Neural Networks\n",
    "\n",
    "In this exercise, you will use PyTorch to build a couple of supervised learning models:\n",
    "* Logistic regression\n",
    "* Shallow neural network\n",
    "* Deep neural network (\"Multilayer Perceptron\")\n",
    "* Convolutional neural network\n",
    "\n",
    "Although PyTorch is a full-fledged deep learning framework with lots of components pre-built (many of them, such as different neural network layers, loss functions, and optimization algorithms in PyTorch's `.nn` submodule), you will build as much as possible from scratch in this exercise.\n",
    "\n",
    "## 1: PyTorch Basics\n",
    "\n",
    "Some of PyTorchs features are very similar to numpy. As you can see in the following code cell, you can define \"arrays\" very similar, but this time they are called [\"Tensor\"](https://en.wikipedia.org/wiki/Tensor_(machine_learning)).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "37bf0275-6ac5-4ad2-bbcf-3e554a9d9920",
   "metadata": {},
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Latex\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "x = torch.Tensor(2, 3, 4)\n",
    "y = torch.zeros(3, 3)\n",
    "\n",
    "print(f\"Using torch {torch.__version__}\")\n",
    "print(f\"x: {x}\\nx.shape: {x.shape}\\ny: {y}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f91c3b42",
   "metadata": {},
   "source": [
    "### 1.1: Basic Operations\n",
    "\n",
    "It is also possible to perform operations on these tensors, often named the same as the numpy methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "08833efd",
   "metadata": {},
   "source": [
    "x1 = torch.ones(3, 4)\n",
    "x2 = torch.arange(3 * 4).reshape(3, 4)\n",
    "sum = x1 + x2\n",
    "\n",
    "print(f\"  {x1}\\n+ {x2}\\n= {sum}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7d09756",
   "metadata": {},
   "source": [
    "You can also do more performant in-place operations that change the original tensor by adding a ` ` _ ` ` after a method name. \n",
    "\n",
    "Note how we need to explictly convert the data type of one of the two tensors, as this conversion is not done automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f85713b2",
   "metadata": {},
   "source": [
    "x2 = torch.arange(3 * 4).reshape(3, 4)\n",
    "x2.add_(x1.long())\n",
    "print(x2)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "82eeb994",
   "metadata": {},
   "source": [
    "For matrix multiplication, we can use the [@ operator](https://pytorch.org/docs/stable/generated/torch.bmm.html?highlight=bmm#torch.bmm) or the [matmul](https://pytorch.org/docs/stable/generated/torch.matmul.html) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "faa76f38",
   "metadata": {},
   "source": [
    "x3 = torch.tile(torch.arange(1, 4), (4, 1)).T\n",
    "x4 = torch.arange(3 * 4).reshape(4, 3)\n",
    "mult = x3 @ x4\n",
    "print(f\"  {x3}\\n@ {x4}\\n= {mult}\\n= {torch.matmul(x3, x4)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f474422b",
   "metadata": {},
   "source": [
    "### 1.2: Gradients\n",
    "\n",
    "Another very important feature of PyTorch is the possibilty to calculate gradients (multiple values) and the derivative (single value) of functions we define. \n",
    "\n",
    "This is very useful when performing gradient descent for the backpropagation algorithm (for weight update in neural networks).\n",
    "\n",
    "All tensors need to be of dtype float and need to have \"requires_grad\" enabled.\n",
    "\n",
    "Let's do a simple example to understand the math behind this. You remember from school that the derivation of $a\\cdot x^n $ is $ n\\cdot a\\cdot x^{n-1}$, so let's use\n",
    "\n",
    "$$ f(x) = 4x^2 $$\n",
    "$$ f'(x)=2 \\cdot  4 \\cdot  x^1 = 8x$$ \n",
    "\n",
    "Given e.g. $x=5$ we can now easily calculate by hand: \n",
    "$$f(5)=4\\cdot 5^2=100$$\n",
    "$$f'(5)=8\\cdot 5=40$$\n",
    "\n",
    "With PyTorch, we can calculate $f'(5)$  without even specifying the $f'(x)$ function first:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "31958624",
   "metadata": {},
   "source": [
    "x5 = torch.tensor(5.0, requires_grad=True)\n",
    "fx5 = 4 * x5**2\n",
    "fx5.backward()\n",
    "\n",
    "display(Latex(f\"$$ f({x5})={ fx5 } \\\\\\\\ f'({x5})={ x5.grad } $$\"))\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c60d20c8",
   "metadata": {},
   "source": [
    "Partial derivatives can be calculated very similar:\n",
    "$$ f(u, v) = 2u^4 + 3v^3 + 2uv $$\n",
    "\n",
    "$$f'_u({u}, {v})=\\frac{\\partial f(u, v)}{\\partial u} = 8u^3 + 2v$$\n",
    "$$f'_v({u}, {v})=\\frac{\\partial f(u, v)}{\\partial v} = 9v^2 + 2u$$\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f6b2cf00",
   "metadata": {},
   "source": [
    "u = torch.tensor(3.0, requires_grad=True)\n",
    "v = torch.tensor(4.0, requires_grad=True)\n",
    "fuv = 2 * u**4 + 3 * v**3 + 2 * u * v\n",
    "fuv.backward()\n",
    "\n",
    "display(\n",
    "    Latex(\n",
    "        f\"$$ f({u},{v})={ fuv }       \\\\\\\\\"\n",
    "        f\"   f'_u({u},{v})={ u.grad } \\\\\\\\\"\n",
    "        f\"   f'_v({u},{v})={ v.grad } $$\"\n",
    "    )\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "612a6acb",
   "metadata": {},
   "source": [
    "We can apply our function to multiple values at once using non-scalar tensors.\n",
    "\n",
    "Just like our good old graphical calculator did it back in the day, we can now calculate the derivation function values for multiple values of x.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fe0893a9",
   "metadata": {},
   "source": [
    "def plt_derivation(x, y, x_label=\"X\"):\n",
    "    plt.plot(x.detach().numpy(), y.detach().numpy(), label=f\"$f({x_label})$\")\n",
    "    plt.plot(x.detach().numpy(), x.grad.detach().numpy(), label=f\"$f'({x_label})$\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(x_label)\n",
    "    plt.gca().spines[\"left\"].set_position(\"zero\")\n",
    "    plt.gca().spines[\"bottom\"].set_position(\"zero\")\n",
    "    plt.gca().spines[\"right\"].set_color(\"none\")\n",
    "    plt.gca().spines[\"top\"].set_color(\"none\")\n",
    "    plt.grid()\n",
    "\n",
    "    return plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "24da230d",
   "metadata": {},
   "source": [
    "sample_range = 10\n",
    "X5 = torch.linspace(-sample_range, sample_range, 21, requires_grad=True)\n",
    "fX5 = 4 * X5**2\n",
    "fX5.sum().backward()\n",
    "\n",
    "plt_derivation(X5, fX5, x_label=\"X_5\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9ea104f",
   "metadata": {},
   "source": [
    "We can use the same approach to plot and calculate gradients of our more advanced function that takes two variables and has two derivations.\n",
    "\n",
    "Note how we create a grid of (repeated) U and V values (feel free to print their values) to properly vectorize the ` ` fUV ` ` function (apply it over all combinations of $u$ and $v$ values).\n",
    "\n",
    "To keep the gradient backtracking functionality, we are not allowed to use any non-PyTorch methods or for example regular ` `for` `-loops. This way, calculation will also be a lot faster.\n",
    "\n",
    "By default, gradients will only be calculated for the starting nodes (\"leafs\") of the \"computation graph\" that PyTorch creates (U and V in this case). If we want to take a look at gradients of intermediate variables, we need to explicitly tell PyTorch to retain the calculated gradient for this variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "c8e8f4f8",
   "metadata": {},
   "source": [
    "def plt_derivation_3d(u, v, y, u_label=\"U\", v_label=\"V\"):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "    # ax.set_proj_type('ortho') # ax.view_init(elev=0, azim=-90, roll=0)\n",
    "    for z_label, Z in {\"f\": y, \"f'_u\": u.grad, \"f'_v\": v.grad}.items():\n",
    "        s = ax.plot_surface(\n",
    "            X=u.detach().numpy(),\n",
    "            Y=v.detach().numpy(),\n",
    "            Z=Z.detach().numpy(),\n",
    "            label=f\"${z_label}({u_label},{v_label})$\",\n",
    "            alpha=0.4,\n",
    "        )\n",
    "        # quick fix for bug in some matplotlib versions:\n",
    "        s._facecolors2d, s._edgecolors2d = s._facecolor3d, s._edgecolor3d\n",
    "    ax.legend()\n",
    "    ax.update({\"xlabel\": u_label, \"ylabel\": v_label, \"zlabel\": \"f\"})\n",
    "    return plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63617193",
   "metadata": {},
   "source": [
    "sample_range = 5\n",
    "U = torch.linspace(-sample_range, sample_range, 21, requires_grad=True)\n",
    "V = torch.linspace(-sample_range, sample_range, 21, requires_grad=True)\n",
    "\n",
    "U_grid, V_grid = torch.meshgrid((U, V), indexing=\"xy\")\n",
    "U_grid.retain_grad()\n",
    "V_grid.retain_grad()\n",
    "\n",
    "fUV = 2 * U_grid**4 + 3 * V_grid**3 + 2 * U_grid * V_grid\n",
    "fUV.sum().backward()\n",
    "\n",
    "# plt_derivation_3d(U_grid, V_grid, fUV)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aac54773",
   "metadata": {},
   "source": [
    "We can use this \"autograd\" functionality for any kind of tensors and chain operations as we want.\n",
    "\n",
    "As you see, with every operation, a function ` ` grad_fn ` ` is passed to the resulting tensor, allowing us to perform the backwards step starting from the last element:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "68dfb569",
   "metadata": {},
   "source": [
    "x6 = torch.arange(3, dtype=torch.float32, requires_grad=True)\n",
    "x7 = torch.arange(4, 7, dtype=torch.float32).requires_grad_()\n",
    "\n",
    "a1 = x6 + x7\n",
    "a2 = a1 + 2\n",
    "a3 = a2**x6\n",
    "print(f\"  {x6}\\n+ {x7}\\n= {a1}\\n+ 2\\n= {a2}\\n^ {x6}\\n= {a3}\")\n",
    "\n",
    "a3.sum().backward()\n",
    "print(\"-\" * 53 + f\"\\ngrad(x6) = {x6.grad}\\ngrad(x7) = {x7.grad}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f841b220",
   "metadata": {},
   "source": [
    "### 1.3: ✏️ Gradient Exercises\n",
    "\n",
    "1. Calculate the derivative $f'(x)$ and $f'(4)$ of $ f(x) = 8x^4 + 4x^2 + x $ by hand using no code.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a806ad82",
   "metadata": {},
   "source": [
    "# TODO: Answer\n",
    "# f'(x) = 32x^3 + 8x + 1\n",
    "# f'(4) = 32 * 4^3 + 8 * 4 + 1\n",
    "# f'(4) = 2081"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de8fbe8e",
   "metadata": {},
   "source": [
    "2. Implement $f(x)$ as a python method and calculate the derivative $f'(4)$ using PyTorch.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Implement\n",
    "def f(x):\n",
    "    return 8 * x**4 + 4 * x**2 + x\n",
    "\n",
    "x = torch.tensor(4.0, requires_grad=True)\n",
    "fx = f(x)\n",
    "f4 = f(4)\n",
    "f_derivat = fx.sum().backward()\n",
    "print(f\"f(4) = {f4}, f'(4) = {x.grad}\")"
   ],
   "id": "94b11415",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. Plot $f(x)$ and $f'(x)$ for $x\\in[-15, 30]$.\n",
   "id": "2fd51d4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Implement\n",
    "x4 = torch.linspace(-15, 30, 28, requires_grad=True)\n",
    "fx = f(x4)\n",
    "fx.sum().backward()\n",
    "\n",
    "plt_derivation(x4, fx, 'X')\n"
   ],
   "id": "5cd5884d",
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3122c007-f55b-4713-802c-6559d941e96c",
   "metadata": {},
   "source": [
    "## 2: Classifying the Iris Dataset\n",
    "\n",
    "Using the basics learned above, we can now load the Iris dataset into PyTorch tensors.\n",
    "\n",
    "### 2.1: ✏️ Load and Preprocess the Dataset\n",
    "\n",
    "1. Load the Iris dataset and modify it to only seperate if something is of class _setosa_ or not. Call this _binary iris_ dataset _biris_ and the variables ``biris_X``, and ``biris_y``. Create a list ``biris_data`` of tuples containing the X values and the corresponding true/false label for each row.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "72eb7477",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "biris = pd.read_csv(\n",
    "    \"data/iris.data\",\n",
    "    header=None,\n",
    "    sep=\",\",\n",
    "    names=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"class\"],\n",
    ")\n",
    "\n",
    "biris[\"biris_y\"] = biris[\"class\"].apply(lambda flower_class: 1 if \"setosa\" in flower_class else 0)\n",
    "biris_X = biris.drop(columns=[\"class\", \"biris_y\"]).values\n",
    "biris_y = biris[\"biris_y\"].values\n",
    "\n",
    "biris_data = list(zip(biris_X, biris_y))\n",
    "print(biris_data[0:5])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5dc2fff1",
   "metadata": {},
   "source": [
    "2. Convert ``biris_X`` and ``biris_y`` to PyTorch tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "903fa34b",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "biris_X = torch.from_numpy(biris_X)\n",
    "biris_y = torch.from_numpy(biris_y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60cbf1a1",
   "metadata": {},
   "source": [
    "### 2.2: ✏️ Logistic Regression\n",
    "\n",
    "We want to perform a binary classification using a logistic regression and the sigmoid function. In a logistic regression, for each feature weights are learned.\n",
    "\n",
    "1. Define a python method ``sigmoid(x)`` that implements the sigmoid function (using PyTorch methods). Compare it to the ``torch.sigmoid`` method for values $x\\in[-5,5]$\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ca5f3d8377f39a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T15:23:52.241649Z",
     "start_time": "2025-05-19T15:23:52.234614Z"
    }
   },
   "source": [
    "# TODO: Implement\n",
    "# sigmoid function 1/1+e^(-x)\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ],
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = torch.linspace(-5, 5, 100)\n",
    "plt.plot(x.detach().numpy(), sigmoid(x).detach().numpy(), label=\"sigmoid\")\n",
    "plt.plot(x.detach().numpy(), torch.sigmoid(x).detach().numpy(), label=\"torch.sigmoid\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Sigmoid\")\n",
    "plt.title(\"Sigmoid function\")\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "id": "cc7cfd0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fa95f80",
   "metadata": {},
   "source": [
    "2. For the logistic regression, we will need to pass both weights and data to the logistic function. Define a method ``logistic(w, x)`` that passes the result of a matrix multiplication of data and weights (``x @ w``) to the sigmoid function.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "99974da9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T15:24:10.363050Z",
     "start_time": "2025-05-19T15:24:10.357929Z"
    }
   },
   "source": [
    "# TODO: Implement\n",
    "def logistic(w, x):\n",
    "    return sigmoid(x @ w)"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35d2f606",
   "metadata": {},
   "source": [
    "3. Call the logistic function using ``biris_X`` and random weights. Save the predictions in the variable ``y_pred_random``.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3450b2b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T15:27:20.810369Z",
     "start_time": "2025-05-19T15:27:20.795615Z"
    }
   },
   "source": [
    "# TODO: Implement\n",
    "y_pred_random = []\n",
    "for i in biris_X:\n",
    "    random_w = torch.randn(4, requires_grad=True, dtype=torch.float64)\n",
    "    y_pred_random.append(logistic(random_w, i))"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba7e2fa6",
   "metadata": {},
   "source": [
    "4. Of course, in reality, the weights should be updated in an iterative process. For this, we will need a loss function. Implement the binary cross entropy loss $$ L_{BCE} = - \\frac{1}{n} \\sum^n_{i=1}\\left(Y_i\\cdot \\log\\hat{Y_i} + (1-Y_i) \\cdot \\log(1-\\hat{Y_i})\\right) $$ (with the predicted labels $\\hat{Y}$ and the groundtruth labels $Y$) in a python method ``bce(y, y_hat)`` using PyTorch methods. Make use of built-in aggregation methods and vectorized methods instead of using ``for``-loops. Note: As calculating the BCE loss is known to be numerically unstable, clamp the output of each $\\log$ to be $\\geq - 100$ (read more [here](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html))."
   ]
  },
  {
   "cell_type": "code",
   "id": "d9ae0b1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T15:27:22.157095Z",
     "start_time": "2025-05-19T15:27:22.149571Z"
    }
   },
   "source": [
    "# TODO: Implement\n",
    "def bce(y, y_hat):\n",
    "    loss = -1 / len(y) * torch.sum(\n",
    "        y * torch.clamp(torch.log(y_hat), min=-100) + (1 - y) * torch.clamp(torch.log(1 - y_hat), min=-100)\n",
    "    )\n",
    "    return loss"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7200f258",
   "metadata": {},
   "source": [
    "5. Evaluate the prediction ``y_pred_random`` using the bce loss function. \n",
    "Compare your own loss value to the output of the method ``torch.nn.functional.binary_cross_entropy(Y_hat, Y)`` (which requires ``Y`` to be of type float).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6def519d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T15:27:23.582596Z",
     "start_time": "2025-05-19T15:27:23.571074Z"
    }
   },
   "source": [
    "# TODO: Implement\n",
    "y_pred_random = torch.stack(y_pred_random)\n",
    "loss_with_random_weights = bce(biris_y, y_pred_random)\n",
    "\n",
    "print(f\"Loss with random weights: {loss_with_random_weights}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with random weights: 2.92714897791467\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "405beb4e",
   "metadata": {},
   "source": [
    "6. Create a new random weight tensor ``w``, but this time enable ``requires_grad``. For the given ``biris_X``, ``biris_y`` and the random weigths ``w`` create a combined loss function $L(X, y, w)$, similar to $f(U,V)$ earlier. Call ``backward()`` on this method instance and print partial derivative $L'_w(X, Y, w) = $ ``w.grad``.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8fd454c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T15:26:14.604304Z",
     "start_time": "2025-05-19T15:26:14.596148Z"
    }
   },
   "source": [
    "# TODO: Implement\n",
    "# sample_range = 5\n",
    "# U = torch.linspace(-sample_range, sample_range, 21, requires_grad=True)\n",
    "# V = torch.linspace(-sample_range, sample_range, 21, requires_grad=True)\n",
    "#\n",
    "# U_grid, V_grid = torch.meshgrid((U, V), indexing=\"xy\")\n",
    "# U_grid.retain_grad()\n",
    "# V_grid.retain_grad()\n",
    "#\n",
    "# fUV = 2 * U_grid**4 + 3 * V_grid**3 + 2 * U_grid * V_grid\n",
    "# fUV.sum().backward()\n",
    "# TODO this is returning none for some reason?\n",
    "w = torch.randn(4, requires_grad=True, dtype=torch.float64)\n",
    "def L(X,y,w):\n",
    "    y_hat = logistic(w, X)\n",
    "    loss = bce(y, y_hat)\n",
    "    return loss\n",
    "\n",
    "w_grad = L(biris_X, biris_y, w).backward()\n",
    "print(w_grad)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65fa39a1",
   "metadata": {},
   "source": [
    "7. Perform a single weight update step by subtracting the calculated gradient from the previous weight wector w. Predict ``y_pred_learned`` by running the logistic function with the updated weights. Evaluate the predictions using the bce loss function. If the loss increases, try multiplying the gradient with a learning rate, e.g. 0.001 before subtracting."
   ]
  },
  {
   "cell_type": "code",
   "id": "2d1aa94a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T18:27:14.400514Z",
     "start_time": "2025-05-19T18:27:14.393040Z"
    }
   },
   "source": [
    "# TODO: Implement\n",
    "y_pred_learned = logistic(w, biris_X)\n"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbd4d698",
   "metadata": {},
   "source": [
    "8. To perform multiple weight update steps, the gradient descent algorithm is used. Run the given method with the fixed learning rate $\\alpha=0.01$ and randomly initialized weights to retrieve learned weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "bd9faa2f",
   "metadata": {},
   "source": [
    "def gd(modelfunc, lossfunc, X, y, w0, alpha, max_epochs=500):\n",
    "    \"\"\"Gradient Descent Algorithm\n",
    "\n",
    "    This basic method is given here, you can implement and improve it by yourself in the optimization lab.\n",
    "\n",
    "    For each epoch:\n",
    "        - forward-step (predict labels and calculate loss)\n",
    "        - backward-step (perform backpropagation)\n",
    "        - update weights (create new weight tensor with reset gradient tracking)\n",
    "        - log weight history\n",
    "\n",
    "    \"\"\"\n",
    "    w = w0.clone().detach().requires_grad_()\n",
    "    for k in range(max_epochs):\n",
    "        pred = modelfunc(w, X)\n",
    "        loss = lossfunc(y, pred)\n",
    "        loss.backward()\n",
    "        direction_of_descent = -w.grad\n",
    "        w = w + alpha * direction_of_descent\n",
    "        w.grad = None\n",
    "        w.retain_grad()\n",
    "    return w\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36904c8b",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "random_weights = torch.randn(4, requires_grad=True, dtype=torch.float64)\n",
    "learned_weights = gd(logistic, bce, biris_X, biris_y, random_weights, alpha=0.01)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81eb0248",
   "metadata": {},
   "source": [
    "9. Use the gradient-descent learned weights to predict ``y_pred_gd_``. Evaluate using the bce loss function (the error should be even lower).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "229eaefc",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "y_pred_gd_ = []\n",
    "for i in biris_X:\n",
    "    y_pred_gd_.append(logistic(learned_weights, i))\n",
    "y_pred_gd_ = torch.stack(y_pred_gd_)\n",
    "\n",
    "loss_with_learned_weights = bce(biris_y, y_pred_gd_)\n",
    "\n",
    "print(f\"Loss with learned weights: {loss_with_learned_weights} | Loss with random weights: {loss_with_random_weights}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3920b90c",
   "metadata": {},
   "source": [
    "10. When you round the output of the logistic function to either 0 or 1, you get the predicted class. Calculate the prediction accuracy for each set of predictions we had so far (``y_pred_random``, ``y_pred_learned`` and ``y_pred_gd``)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "babd3a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T18:27:07.435553Z",
     "start_time": "2025-05-19T18:27:07.353161Z"
    }
   },
   "source": [
    "# TODO: Implement\n",
    "def accuracy(y, y_hat):\n",
    "    y_hat = torch.round(y_hat)\n",
    "    correct = torch.sum(y == y_hat).item()\n",
    "    return correct / len(y)\n",
    "\n",
    "print(f\"Accuracy with random weights: {accuracy(biris_y, y_pred_random)} | Accuracy with learned weights: {accuracy(biris_y, y_pred_learned)} | Accuracy with GD weights: {accuracy(biris_y, y_pred_gd_)}\")"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_learned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m     correct \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(y \u001B[38;5;241m==\u001B[39m y_hat)\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m correct \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mlen\u001B[39m(y)\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAccuracy with random weights: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy(biris_y,\u001B[38;5;250m \u001B[39my_pred_random)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Accuracy with learned weights: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy(biris_y,\u001B[38;5;250m \u001B[39my_pred_learned)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m | Accuracy with GD weights: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00maccuracy(biris_y,\u001B[38;5;250m \u001B[39my_pred_gd_)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'y_pred_learned' is not defined"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ae88e02",
   "metadata": {},
   "source": [
    "11. Write a new gradient descent method ``gdi`` that saves the weights and the loss in each epoch (and returns lists of both in the end).\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "731f4148",
   "metadata": {},
   "source": [
    "def gdi(modelfunc, lossfunc, X, y, w0, alpha, max_epochs=500):\n",
    "    # TODO: Implement\n",
    "    w_history = []\n",
    "    l_history = []\n",
    "    w = w0.clone().detach().requires_grad_()\n",
    "    for k in range(max_epochs):\n",
    "        pred = modelfunc(w, X)\n",
    "        loss = lossfunc(y, pred)\n",
    "        loss.backward()\n",
    "        direction_of_descent = -w.grad\n",
    "        w = w + alpha * direction_of_descent\n",
    "        w.grad = None\n",
    "        w.retain_grad()\n",
    "        w_history.append(w.clone().detach())\n",
    "        l_history.append(loss.item())\n",
    "    return w_history, l_history"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81622be5",
   "metadata": {},
   "source": [
    "12. Write a method ``visualize_error(errors)`` to visualize the loss in each epoch. Train ``gdi`` for 1000 epochs on biris and plot the error for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ad10bf39",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "def visualize_error(errors):\n",
    "    plt.plot(errors)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss over epochs\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "_, l_history = gdi(logistic,bce, biris_X, biris_y, random_weights, alpha=0.01)\n",
    "visualize_error(l_history)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "656c5dbf",
   "metadata": {},
   "source": [
    "13. Train 50 epochs of gdi for different learning rates (at least 3), which ones work better than others? Create a method ``visualize_loss_multi`` that takes a list of tuples ``(lr, loss_history)`` as input and visualize the different loss curves.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "082b0095",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "def visualize_loss_multi(learn_history):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(len(learn_history)):\n",
    "        plt.plot(learn_history[i][1], label=f\"lr={learn_history[i][0]}\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Loss over epochs\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "lr_list = [0.001, 0.01, 0.1]\n",
    "learn_history = []\n",
    "for lr in lr_list:\n",
    "    _, l_history = gdi(logistic, bce, biris_X, biris_y, random_weights, lr, max_epochs=50)\n",
    "    learn_history.append([lr, l_history])\n",
    "visualize_loss_multi(learn_history)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfa01dd8-5352-478a-a78c-916612aef99c",
   "metadata": {},
   "source": [
    "### 2.3: ✏️ Shallow neural network for Iris Dataset\n",
    "\n",
    "A shallow neural network has exactly one hidden (fully-connected) layer between the input and output layers.\n",
    "\n",
    "We therefore train two \"layers\" of weights, one connecting each input feature to each neuron of the hidden layer and one connecting each hidden layer neuron to the final output layer, which in our case consists of only one neuron."
   ]
  },
  {
   "cell_type": "code",
   "id": "55a674d1",
   "metadata": {},
   "source": [
    "n_hidden_neuron = 10\n",
    "n_features = biris_X.shape[1]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e7ac54b",
   "metadata": {},
   "source": [
    "1. Initialize the two random weight tensors ``w1`` and ``w2``. Note: These `w` matrices need to be transposed compared to the lecture. Here, `n_features` is the number of rows of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "id": "b5e9f9e4",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "w1 = torch.randn(n_hidden_neuron, requires_grad=True, dtype=torch.float64)\n",
    "w1 = w1.T\n",
    "w2 = torch.randn(n_hidden_neuron, requires_grad=True, dtype=torch.float64)\n",
    "w2 = w2.T"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7f13b34",
   "metadata": {},
   "source": [
    "2. Define a python method to calculate the outputs of the neural network (predictions). \n",
    "This is quite easy, just predict like you previously did, but chain the output of the first layer to the second layer and use w1 and w2 respectively. \n",
    "[Squeeze](https://pytorch.org/docs/stable/generated/torch.squeeze.html) the output to remove unnecessary lists."
   ]
  },
  {
   "cell_type": "code",
   "id": "1c8279df",
   "metadata": {},
   "source": [
    "def nn_shallow(w1, w2, x):\n",
    "    return\n",
    "    # TODO: Implement\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9cd28f0f",
   "metadata": {},
   "source": [
    "3. Run your shallow network on biris and the two random weights. Print the output shape"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f38a2d0",
   "metadata": {},
   "source": [
    "# TODO: Implement\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46599334",
   "metadata": {},
   "source": [
    "4. Calculate BCE loss on these predictions"
   ]
  },
  {
   "cell_type": "code",
   "id": "f34f7eb3",
   "metadata": {},
   "source": [
    "# TODO: Implement\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38d8c495",
   "metadata": {},
   "source": [
    "5. Create a copy of the gradient descend ``gdi`` method called ``gdi_two_w`` to allow updating both weights w1 and w2 (using the respective gradients). It will return the history for both weights separately."
   ]
  },
  {
   "cell_type": "code",
   "id": "2ca8d266",
   "metadata": {},
   "source": [
    "# TODO: Implement\n",
    "def gdi_two_w(modelfunc, lossfunc, X, y, w1_0, w2_0, alpha, max_epochs=500):\n",
    "    w1_history = []\n",
    "    w2_history = []\n",
    "    l_history = []\n",
    "    return w1_history, w2_history, l_history"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4869056",
   "metadata": {},
   "source": [
    "6. Initialize random weights with ``requires_grad=True`` and train 1000 epochs of ``gdi_two_w`` using your shallow network as ``modelfunc``, the biris dataset and a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "id": "ee75db39",
   "metadata": {},
   "source": [
    "# TODO: Implement\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ad319e2",
   "metadata": {},
   "source": [
    "7. Train 120 epochs of ``gdi_two_w`` for different learning rates (at least 3). Does the error always go down with more epochs? Which learning rates work well?"
   ]
  },
  {
   "cell_type": "code",
   "id": "4bcec703",
   "metadata": {},
   "source": [
    "# TODO: Implement & Answer\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0650640",
   "metadata": {},
   "source": [
    "## 3: Classifying the Fashion MNIST Dataset\n",
    "So far, we build everything by ourselves. PyTorch however also offers a wide variety of functions to define neural networks easier.\n",
    "In this example, we create a PyTorch Module to define a simple neural network that will classify the fashion mnist data.\n",
    "\n",
    "### 3.1: Load the Data into a tensor\n",
    "Just as in the PCA exercise, we first load the data from disk into a dataframe and quickly visualize it."
   ]
  },
  {
   "cell_type": "code",
   "id": "d1eed4df",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "fm = pd.read_csv(\"data/fashion_mnist_train.csv.zip\", index_col=\"Id\")\n",
    "fm_labels = [\"T-Shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Boot\"]\n",
    "display(fm.head(2))\n",
    "\n",
    "# Visualize Examples\n",
    "num_samples = 20\n",
    "ax = plt.subplots(nrows=1, ncols=num_samples, figsize=(24, 4))[1]\n",
    "for i in range(num_samples):\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].set_title(f\"{i}\\n{fm_labels[fm.Category[i]]}\")\n",
    "    ax[i].imshow(fm.loc[i].values[1:].reshape((28,28)), cmap=\"gray_r\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e34549d",
   "metadata": {},
   "source": [
    "Now, we transform it into PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "id": "78f55490",
   "metadata": {},
   "source": [
    "fm_X = torch.tensor(fm.drop(\"Category\", axis=1).values, dtype=torch.float32).reshape(-1, 1, 28, 28)\n",
    "fm_y = torch.tensor(fm.Category.values.astype('int') , dtype=torch.long)\n",
    "\n",
    "print(fm_X.shape, fm_y.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1c1fa529",
   "metadata": {},
   "source": [
    "### 3.2: Defining and Training a simple Classifier\n",
    "Defining a neural network in PyTorch using the Module API will look like this: "
   ]
  },
  {
   "cell_type": "code",
   "id": "e60afd87",
   "metadata": {},
   "source": [
    "class SimpleClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_outputs):\n",
    "        super().__init__()\n",
    "        # self.linear1 = nn.Linear(num_inputs, num_hidden_neurons)\n",
    "        # self.linear2 = torch.nn.Linear(num_hidden_neurons, num_outputs)\n",
    "        self.conv1 = torch.nn.Conv2d(1, 4, 3, 1)\n",
    "        self.conv2 = torch.nn.Conv2d(4, 8, 3, 1)\n",
    "        self.dropout1 = torch.nn.Dropout(0.25)\n",
    "        self.dropout2 = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(1152, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, num_outputs)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = torch.nn.functional.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.nn.functional.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = torch.nn.functional.log_softmax(x, dim=1)\n",
    "        return output\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a7b6b29",
   "metadata": {},
   "source": [
    "Our training loop will look like this:"
   ]
  },
  {
   "cell_type": "code",
   "id": "55c40334",
   "metadata": {},
   "source": [
    "def train_model(model, optimizer, X, y, loss_module, num_epochs=5):\n",
    "    model.train()\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        preds = model(X)\n",
    "        preds = preds.squeeze(dim=1)\n",
    "        loss = loss_module(preds, y)\n",
    "        loss_history.append(loss.detach().numpy().item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"loss after epoch {epoch}: {loss}\")\n",
    "    return loss_history"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c9daba8c",
   "metadata": {},
   "source": [
    "For running our training loop, we simply need to populate all the method parameters. For optimizer and loss, we can simply choose from what's built-in. Training the neural network will take about 3 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "id": "115b107b",
   "metadata": {},
   "source": [
    "model = SimpleClassifier(num_outputs=10)\n",
    "print(model)\n",
    "\n",
    "loss_history = train_model(\n",
    "    model,\n",
    "    optimizer=torch.optim.SGD(model.parameters(), lr=0.1), # instead of GD\n",
    "    X=fm_X,\n",
    "    y=fm_y,\n",
    "    loss_module=torch.nn.CrossEntropyLoss(), # instead of BinaryCrossEntropy\n",
    "    num_epochs=10\n",
    ")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04c441f5",
   "metadata": {},
   "source": [
    "Everything the model learned is now contained in the ``model`` object, including weights.\n",
    "The history of losses was saved in our training loop."
   ]
  },
  {
   "cell_type": "code",
   "id": "e5a856ec",
   "metadata": {},
   "source": [
    "print(list(model.state_dict().keys()), model.state_dict()[\"conv1.weight\"][0][0][0])\n",
    "print(loss_history)\n",
    "\n",
    "\n",
    "visualize_loss(loss_history)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ef389f5",
   "metadata": {},
   "source": [
    "### 4: ✏️ PyTorch model for Iris (Optional)\n",
    "1. Create a PyTorch classifier class, model and training function for classifying either Iris or Biris. "
   ]
  },
  {
   "cell_type": "code",
   "id": "eea6afd0",
   "metadata": {},
   "source": [
    "# TODO: Implement (optional)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f97fd0d",
   "metadata": {},
   "source": [
    "2. Compare your accuracy to the previous approach"
   ]
  },
  {
   "cell_type": "code",
   "id": "80008898",
   "metadata": {},
   "source": [
    "# TODO: Implement (optional)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
